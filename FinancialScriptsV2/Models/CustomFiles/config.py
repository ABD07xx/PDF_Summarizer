#Config settings for our open source LLM Model
llm_config = {
    "temperature": 0.2,
    "model_name": "ollama/llama3",
    "api_key": "ollama",
    "base_url": "http://localhost:4000"
}
