#Config settings for our open source LLM Model
llm_config = {
    "temperature": 0.1,
    "model_name": "llama3",
    "api_key": "ollama",
    "base_url": "http://localhost:4000"
}
